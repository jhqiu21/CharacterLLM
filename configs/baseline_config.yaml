model_name: Baseline

model:
  vocab_size: 27
  d_model: 256
  n_heads: 8
  n_layers: 2
  max_len: 128
  mlp_ratio: 4
  emb_dropout: 0.2
  mlp_dropout: 0.2
  attn_dropout: 0.1
  resid_dropout: 0.2

training:
  batch_size: 128
  sequence_length: 128
  learning_rate: 0.0003
  epochs: 5000
  eval_interval: 500
  eval_iters: 100
  warmup_iters: 2000
  checkpoint_interval: 5000
  seed: 4212

output:
  checkpoint_dir: ./checkpoints
  results_dir: ./runs
