model_name: Baseline

model:
  vocab_size: 27
  d_model: 256
  n_heads: 4
  n_layers: 4
  max_len: 128
  mlp_ratio: 4
  emb_dropout: 0.1
  mlp_dropout: 0.1
  attn_dropout: 0.1
  resid_dropout: 0.1

loss:
  LABEL_SMOOTHING: True
  ls_eps: 0.05
  # entropy bonus weight
  entropy_lambda: 0.0

training:
  batch_size: 128
  sequence_length: 128
  learning_rate: 0.0003
  epochs: 10000
  eval_interval: 500
  warmup_iters: 2000
  stage_checkpoint_interval: 10000
  seed: 4212

output:
  checkpoint_dir: ./checkpoints
  results_dir: ./runs
