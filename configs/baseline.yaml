model_name: Baseline

model:
  vocab_size: 27
  d_model: 256
  n_heads: 4
  n_layers: 4
  max_len: 128
  mlp_ratio: 4
  emb_dropout: 0.1
  mlp_dropout: 0.1
  attn_dropout: 0.1
  resid_dropout: 0.1
  pos_encoding_type: 'rope'

loss:
  LABEL_SMOOTHING: True
  ls_eps: 0.05
  # entropy bonus weight
  entropy_lambda: 0.0
  # position weights mode, linear, sqrt, none
  tail_scheme: 'linear'

scheduler:
  warmup_iters: 2000
  init_value: 0.0
  end_value: 0.0

training:
  batch_size: 128
  sequence_length: 128
  learning_rate: 0.0003
  epochs: 5000
  validation_interval: 500
  stage_checkpoint_interval: 10000
  seed: 4212

evaluation:
  sample: True
  seed: 4212
  # prompt for text generation
  prompt: "hello my fri"
  # length of text to generate
  gen_len: 1000
  # temperature for text generation
  temperature: 0.7
  self_belu_n_grams: 4
  self_belu_n_samples: 20
  # ngram size for preventing ngram repetition during generation (null to disable)
  # set in transformer.ipynb 

output:
  # directory to save model checkpoints
  checkpoint_dir: ./checkpoints
  # directory to save training and evaluation results
  results_dir: ./runs
