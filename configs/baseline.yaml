model_name: LSTM-baseline

model:
  vocab_size: 27
  hidden_size: 256
  n_heads: 4
  n_layers: 4
  max_len: 128
  dropout: 0.1

loss:
  LABEL_SMOOTHING: True
  ls_eps: 0.05
  # entropy bonus weight
  entropy_lambda: 0.0
  # position weights mode, linear, sqrt, none
  tail_scheme: 'linear'

scheduler:
  warmup_iters: 2000
  init_value: 0.0
  end_value: 0.0

training:
  batch_size: 128
  sequence_length: 128
  learning_rate: 0.0003
  epochs: 5000
  validation_interval: 500
  stage_checkpoint_interval: 10000
  seed: 4212

evaluation:
  sample: True
  seed: 4212
  # prompt for text generation
  prompt: "hello my fri"
  # length of text to generate
  gen_len: 1000
  # temperature for text generation
  temperature: 0.7
  self_belu_n_grams: 4
  self_belu_n_samples: 20
  # ngram size for preventing ngram repetition during generation is set in transformer.ipynb  (null to disable)

output:
  # directory to save model checkpoints
  checkpoint_dir: ./checkpoints
  # directory to save training and evaluation results
  results_dir: ./runs