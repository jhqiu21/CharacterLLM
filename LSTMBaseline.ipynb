{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6d2900",
   "metadata": {},
   "source": [
    "## LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2def0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload of local Python modules (e.g., models)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5001b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import time\n",
    "\n",
    "# local imports\n",
    "import models.LSTM as models\n",
    "import utils.generation as generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906db4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the jax random key\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a1c8f",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86825275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ./data/text8_train.txt and ./data/text8_test.txt files\n",
    "with open(\"./data/text8_train.txt\", \"r\") as f:\n",
    "    train_text = f.read()\n",
    "with open(\"./data/text8_test.txt\", \"r\") as f:\n",
    "    test_text = f.read()\n",
    "\n",
    "# print the length of the training text and test text\n",
    "print(f\"Length of training text: {len(train_text):_} characters\")\n",
    "print(f\"Length of test text: {len(test_text):_} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdca63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary (lowercase + space + a few punctuations)\n",
    "char_set = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "char_to_int = {ch:i for i,ch in enumerate(char_set)}\n",
    "int_to_char = {i:ch for ch,i in char_to_int.items()}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"Encode string to array of integers\"\"\"\n",
    "    ids = [char_to_int[c] for c in s]\n",
    "    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text\n",
    "train_text_int = encode(train_text)\n",
    "test_text_int = encode(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: display a few random characters from the training text\n",
    "T = 128\n",
    "for _ in range(5):\n",
    "    # choose random position in text\n",
    "    N = np.random.randint(low=0, high=len(train_text)-T)\n",
    "    print(train_text[N:N+T])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724c34b",
   "metadata": {},
   "source": [
    "# Create a basic LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5955c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, vocab_size=27, hidden_size=256, n_layers=2, dropout=0.1, max_len=128):\n",
    "    # create a basic LSTM model\n",
    "    model = models.CharLSTM(\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        n_layers,\n",
    "        dropout,\n",
    "        max_len,\n",
    "    )\n",
    "    # create a dummy input for initialization\n",
    "    dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)\n",
    "    # pass the dummy input to the model to initialize the parameters\n",
    "    params = model.init({\"params\": rng, 'dropout': rng}, dummy, train=True)[\"params\"]\n",
    "    return model, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size\n",
    "vocab_size = len(char_set)\n",
    "\n",
    "# internal model dimensions\n",
    "# d_model = 256\n",
    "# dimension of the hidden state in LSTM\n",
    "hidden_size = 256\n",
    "\n",
    "# number of attention heads\n",
    "# n_heads = 8\n",
    "\n",
    "# number of LSTM layers: 1-3 layers are common for LSTM\n",
    "n_layers = 4\n",
    "\n",
    "# maximum sequence length\n",
    "max_len = 128\n",
    "\n",
    "# dropout rate between LSTM layers\n",
    "dropout = 0.1\n",
    "\n",
    "model, params = create_train_state(key, vocab_size, hidden_size, n_layers, dropout, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of parameters\n",
    "def count_params(params):\n",
    "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "\n",
    "print(f\"Number of parameters: {count_params(params):_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52231d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: create a batch of data & run a forward pass\n",
    "B, T = 4, 32\n",
    "batch = jax.random.randint(\n",
    "    key=key,\n",
    "    shape=(B, T), minval=0, maxval=len(char_set))\n",
    "logits = model.apply({\"params\": params}, batch, train=False)\n",
    "\n",
    "print(\"batch shape:\", batch.shape)  # (B, T)\n",
    "print(\"logits shape:\", logits.shape)  # (B, T, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0213425",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_and_metrics(logits, targets):\n",
    "    \"\"\"Compute cross-entropy loss and accuracy.\n",
    "\n",
    "    Assumes `targets` contains only valid integer class ids in [0, V-1] (no -1 ignore tokens).\n",
    "\n",
    "    Args:\n",
    "      logits: (B, T, V) float array of unnormalized scores.\n",
    "      targets: (B, T) integer array with ground-truth class ids.\n",
    "\n",
    "    Returns:\n",
    "      loss: scalar average cross-entropy over all positions.\n",
    "      metrics: dict with keys \"loss\" and \"acc\" (both scalars).\n",
    "    \"\"\"\n",
    "    # Flatten batch/time dims so optax works on shape (N, V) and (N,)\n",
    "    vocab = logits.shape[-1]\n",
    "    flat_logits = logits.reshape(-1, vocab)\n",
    "    flat_targets = targets.reshape(-1)\n",
    "\n",
    "    # Per-position cross-entropy, then mean over all positions\n",
    "    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)\n",
    "    loss = per_pos.mean()\n",
    "\n",
    "    # prediction over all positions\n",
    "    preds = jnp.argmax(logits, axis=-1)  # (B, T)\n",
    "\n",
    "    # compute accuracy over only the last position\n",
    "    is_match = preds == targets\n",
    "\n",
    "    # Accuracy over all positions\n",
    "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
    "\n",
    "    # Accuracy over only last position\n",
    "    acc_last = jnp.mean(is_match.astype(jnp.float32)[:,-1])\n",
    "\n",
    "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05221147",
   "metadata": {},
   "source": [
    "# Optimization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an update function\n",
    "def train_step(params, step_rng, opt_state, x, y, tx):\n",
    "    \"\"\"Single optimization step using optax optimizer.\n",
    "\n",
    "    Args:\n",
    "      params: pytree of model parameters.\n",
    "      opt_state: optax optimizer state corresponding to `params`.\n",
    "      x: (B, T) int array input tokens.\n",
    "      y: (B, T) int array target tokens.\n",
    "      tx: optax.GradientTransformation (already initialized).\n",
    "\n",
    "    Returns:\n",
    "      new_params: updated parameters after one gradient step.\n",
    "      new_opt_state: updated optimizer state.\n",
    "      metrics: dict of scalar metrics (loss, acc).\n",
    "    \"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply({\"params\": params}, x, train=True, rngs={\"dropout\": step_rng})\n",
    "        loss, metrics = loss_and_metrics(logits, y)\n",
    "        return loss, metrics\n",
    "\n",
    "    # compute gradients (loss is scalar, metrics is auxiliary)\n",
    "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "\n",
    "    # optax update: compute parameter updates and new optimizer state\n",
    "    updates, new_opt_state = tx.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, metrics\n",
    "\n",
    "\n",
    "# jit: last argument should be static because it is an object\n",
    "train_step = jax.jit(train_step, static_argnames=(\"tx\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d8e59",
   "metadata": {},
   "source": [
    "# Batch creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb30f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch from the training data\n",
    "def get_batch(text_int, B, T):\n",
    "    \"\"\"Create a random batch of data from text_int.\n",
    "\n",
    "    Args:\n",
    "      text_int: 1D array of token ids.\n",
    "      B: batch size (number of sequences).\n",
    "      T: sequence length (number of tokens per sequence).\n",
    "\n",
    "    Returns:\n",
    "      x: (B, T) int array input tokens.\n",
    "      y: (B, T) int array target tokens.\n",
    "    \"\"\"\n",
    "    # choose random starting indices for each sequence in the batch\n",
    "    ix = np.random.randint(0, len(text_int) - T, size=B)\n",
    "    # inputs are text from i to i+T\n",
    "    x = np.stack([text_int[i:i+T] for i in ix])\n",
    "    # targets are text from i+1 to i+T+1\n",
    "    y = np.stack([text_int[i+1:i+T+1] for i in ix])\n",
    "    return jnp.array(x, dtype=jnp.int32), jnp.array(y, dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da80db",
   "metadata": {},
   "source": [
    "# Optimizer creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 100_000\n",
    "B, T = 128, 128\n",
    "warmup_steps = 2000\n",
    "decay_steps = max(1, niter - warmup_steps)\n",
    "\n",
    "# define optax optimizer\n",
    "learning_rate = 0.0003\n",
    "\n",
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_steps=decay_steps,\n",
    "    end_value=0.0,\n",
    ")\n",
    "\n",
    "# Create Adam optimizer (Optax)\n",
    "# tx = optax.adam(learning_rate=learning_rate)\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),    # Optional gradient clipping\n",
    "    optax.adamw(\n",
    "        learning_rate=lr_schedule,     # ← Use schedule here\n",
    "        b1=0.9,\n",
    "        b2=0.95,\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.1,\n",
    "    ),\n",
    ")\n",
    "# Initialize optimizer state for current params\n",
    "opt_state = tx.init(params)\n",
    "print(f\"Initialized optimizer: Adam lr={learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "time_history = []\n",
    "time_test_history = []\n",
    "loss_test_history = []\n",
    "acc_test_history = []\n",
    "acc_last_test_history = []\n",
    "iteration_history = []\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "best_test_loss_iter = 0\n",
    "best_test_acc = 0\n",
    "best_test_acc_iter = 0\n",
    "best_test_acc_last = 0\n",
    "best_test_acc_last_iter = 0\n",
    "\n",
    "time_start = time.time()\n",
    "for it in range(niter):\n",
    "    key, step_key = jax.random.split(key)\n",
    "    batch = get_batch(train_text_int, B, T)\n",
    "    input, target = batch[0], batch[1]\n",
    "    params_new, opt_state_new, metrics = train_step(params, step_key, opt_state, input, target, tx)\n",
    "\n",
    "    # update params and opt_state\n",
    "    params = params_new\n",
    "    opt_state = opt_state_new\n",
    "    acc = metrics['acc']\n",
    "    acc_last = metrics['acc_last']\n",
    "    loss = metrics['loss']\n",
    "\n",
    "    loss_history.append(loss)\n",
    "    time_history.append(time.time() - time_start)\n",
    "\n",
    "    if it % (niter // 50) == 0 or it == niter - 1:\n",
    "        time_since_start = time.time() - time_start\n",
    "        # compute loss on test set\n",
    "        B_test, T_test = 1024, 32\n",
    "        test_batch = get_batch(test_text_int, B_test, T_test)\n",
    "        test_input, test_target = test_batch[0], test_batch[1]\n",
    "        test_logits = model.apply({\"params\": params}, test_input, train=False)\n",
    "        test_loss, test_metrics = loss_and_metrics(test_logits, test_target)\n",
    "        test_acc = test_metrics['acc']\n",
    "        test_acc_last = test_metrics['acc_last']\n",
    "        \n",
    "        # record test metrics\n",
    "        loss_test_history.append(test_loss)\n",
    "        acc_test_history.append(test_acc)\n",
    "        acc_last_test_history.append(test_acc_last)\n",
    "        time_test_history.append(time_since_start)\n",
    "        iteration_history.append(it)\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_test_loss_iter = it\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_test_acc_iter = it\n",
    "        if test_acc_last > best_test_acc_last:\n",
    "            best_test_acc_last = test_acc_last\n",
    "            best_test_acc_last_iter = it\n",
    "            \n",
    "        print(f\"iteration {it:_}  time: {time_since_start:.1f} seconds\")\n",
    "        print(f\"\\t \\t loss(train :: test): {loss:.4f} :: {test_loss:.4f}\")\n",
    "        print(f\"\\t \\t accuracy (train :: test): {100*acc:.1f}% :: {100*test_acc:.1f}%\")\n",
    "        print(f\"\\t \\t accuracy (last character) (train :: test): {100*acc_last:.1f}% :: {100*test_acc_last:.1f}%\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91a94a",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best performance\n",
    "print(\"\\n Best Performance:\")\n",
    "print(f\"  - Test Loss:          {best_test_loss:.4f} @ iteration {best_test_loss_iter:_}\")\n",
    "print(f\"  - Test Accuracy:      {100*best_test_acc:.2f}% @ iteration {best_test_acc_iter:_}\")\n",
    "print(f\"  - Last Char Accuracy: {100*best_test_acc_last:.2f}% @ iteration {best_test_acc_last_iter:_}\")\n",
    "\n",
    "# Final average (last 5-10 checkpoints)\n",
    "n_final = min(10, len(loss_test_history))\n",
    "final_loss = np.array(loss_test_history[-n_final:])\n",
    "final_acc = np.array(acc_test_history[-n_final:])\n",
    "final_acc_last = np.array(acc_last_test_history[-n_final:])\n",
    "\n",
    "print(f\"\\n Final Average (last {n_final} checkpoints):\")\n",
    "print(f\"  - Test Loss:          {final_loss.mean():.4f} ± {final_loss.std():.4f}\")\n",
    "print(f\"  - Test Accuracy:      {100*final_acc.mean():.2f}% ± {100*final_acc.std():.2f}%\")\n",
    "print(f\"  - Last Char Accuracy: {100*final_acc_last.mean():.2f}% ± {100*final_acc_last.std():.2f}%\")\n",
    "\n",
    "# Training info\n",
    "total_time = time.time() - time_start\n",
    "print(\"\\n  Training Info:\")\n",
    "print(f\"  - Total Time:         {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
    "print(f\"  - Total Iterations:   {niter:_}\")\n",
    "print(f\"  - Time per Iteration: {total_time/niter:.3f} seconds\")\n",
    "\n",
    "# Convergence analysis\n",
    "loss_improvement = (loss_test_history[0] - loss_test_history[-1]) / loss_test_history[0] * 100\n",
    "acc_improvement = (acc_test_history[-1] - acc_test_history[0]) * 100\n",
    "print(\"\\n Convergence:\")\n",
    "print(f\"  - Loss Reduction:     {loss_improvement:.1f}%\")\n",
    "print(f\"  - Accuracy Gain:      {acc_improvement:.1f} percentage points\")\n",
    "\n",
    "# Check if still improving (compare last 20% vs previous 20%)\n",
    "if len(loss_test_history) >= 10:\n",
    "    n_window = len(loss_test_history) // 5\n",
    "    recent_loss = np.mean(loss_test_history[-n_window:])\n",
    "    earlier_loss = np.mean(loss_test_history[-2*n_window:-n_window])\n",
    "    still_improving = recent_loss < earlier_loss\n",
    "    print(f\"  Still Improving:    {'Yes' if still_improving else 'No (plateau)'}\")\n",
    "else:\n",
    "    print(\"  Still Improving:    N/A (insufficient data)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save results to file (optional)\n",
    "save_results = True  # Set to True to save\n",
    "if save_results:\n",
    "    import json\n",
    "    results = {\n",
    "        \"best\": {\n",
    "            \"test_loss\": float(best_test_loss),\n",
    "            \"test_loss_iter\": int(best_test_loss_iter),\n",
    "            \"test_acc\": float(best_test_acc),\n",
    "            \"test_acc_iter\": int(best_test_acc_iter),\n",
    "            \"test_acc_last\": float(best_test_acc_last),\n",
    "            \"test_acc_last_iter\": int(best_test_acc_last_iter),\n",
    "        },\n",
    "        \"final_avg\": {\n",
    "            \"test_loss_mean\": float(final_loss.mean()),\n",
    "            \"test_loss_std\": float(final_loss.std()),\n",
    "            \"test_acc_mean\": float(final_acc.mean()),\n",
    "            \"test_acc_std\": float(final_acc.std()),\n",
    "            \"test_acc_last_mean\": float(final_acc_last.mean()),\n",
    "            \"test_acc_last_std\": float(final_acc_last.std()),\n",
    "            \"n_checkpoints\": int(n_final),\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"total_time\": float(total_time),\n",
    "            \"total_iterations\": int(niter),\n",
    "            \"time_per_iter\": float(total_time/niter),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('training_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(\"\\n Results saved to 'training_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b89b2",
   "metadata": {},
   "source": [
    "## Test Metrics Ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a comprehensive figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ===== Plot 1: Loss Curves =====\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(time_history, loss_history, '-', label='Train', color=\"blue\", alpha=0.6)\n",
    "ax1.plot(time_test_history, loss_test_history, '-', label='Test', lw=2, color=\"red\")\n",
    "# Mark best test loss\n",
    "best_idx = np.argmin(loss_test_history)\n",
    "ax1.scatter(time_test_history[best_idx], loss_test_history[best_idx],\n",
    "           s=100, color='red', marker='*', zorder=5,\n",
    "           label=f'Best: {loss_test_history[best_idx]:.4f}')\n",
    "ax1.set_xlabel(\"Time (seconds)\", fontsize=11)\n",
    "ax1.set_ylabel(\"Loss\", fontsize=11)\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_title(\"Training & Test Loss\", fontsize=12, fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# ===== Plot 2: Accuracy Curves =====\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(iteration_history, [100*x for x in acc_test_history], '-o',\n",
    "        lw=2, markersize=3, color=\"green\", label='Test Acc')\n",
    "# Mark best accuracy\n",
    "best_acc_idx = np.argmax(acc_test_history)\n",
    "ax2.scatter(iteration_history[best_acc_idx], 100*acc_test_history[best_acc_idx],\n",
    "           s=100, color='green', marker='*', zorder=5,\n",
    "           label=f'Best: {100*acc_test_history[best_acc_idx]:.2f}%')\n",
    "# Add final average line\n",
    "n_final = min(10, len(acc_test_history))\n",
    "final_avg = np.mean(acc_test_history[-n_final:]) * 100\n",
    "ax2.axhline(y=final_avg, color='orange', linestyle='--', alpha=0.7,\n",
    "           label=f'Final Avg: {final_avg:.2f}%')\n",
    "ax2.set_xlabel(\"Iteration\", fontsize=11)\n",
    "ax2.set_ylabel(\"Test Accuracy (%)\", fontsize=11)\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.set_title(\"Test Accuracy\", fontsize=12, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# ===== Plot 3: Last Character Accuracy =====\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(iteration_history, [100*x for x in acc_last_test_history], '-s',\n",
    "        lw=2, markersize=3, color=\"purple\", label='Last Char Acc')\n",
    "# Mark best\n",
    "best_last_idx = np.argmax(acc_last_test_history)\n",
    "ax3.scatter(iteration_history[best_last_idx], 100*acc_last_test_history[best_last_idx],\n",
    "           s=100, color='purple', marker='*', zorder=5,\n",
    "           label=f'Best: {100*acc_last_test_history[best_last_idx]:.2f}%')\n",
    "# Add final average line\n",
    "final_last_avg = np.mean(acc_last_test_history[-n_final:]) * 100\n",
    "ax3.axhline(y=final_last_avg, color='orange', linestyle='--', alpha=0.7,\n",
    "           label=f'Final Avg: {final_last_avg:.2f}%')\n",
    "ax3.set_xlabel(\"Iteration\", fontsize=11)\n",
    "ax3.set_ylabel(\"Last Character Accuracy (%)\", fontsize=11)\n",
    "ax3.legend(loc='lower right')\n",
    "ax3.set_title(\"Last Character Prediction\", fontsize=12, fontweight='bold')\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# ===== Plot 4: Loss Convergence (smoothed) =====\n",
    "ax4 = axes[1, 1]\n",
    "# Smooth the test loss curve\n",
    "window_size = max(1, len(loss_test_history) // 10)\n",
    "if window_size > 1:\n",
    "    smoothed_loss = np.convolve(loss_test_history,\n",
    "                                np.ones(window_size)/window_size,\n",
    "                                mode='valid')\n",
    "    smoothed_iters = iteration_history[window_size-1:]\n",
    "    ax4.plot(smoothed_iters, smoothed_loss, '-', lw=2.5, color='darkred',\n",
    "            label=f'Smoothed (window={window_size})')\n",
    "ax4.plot(iteration_history, loss_test_history, '-', alpha=0.3, color='red',\n",
    "        label='Raw Test Loss')\n",
    "ax4.set_xlabel(\"Iteration\", fontsize=11)\n",
    "ax4.set_ylabel(\"Test Loss\", fontsize=11)\n",
    "ax4.legend(loc='upper right')\n",
    "ax4.set_title(\"Loss Convergence (Smoothed)\", fontsize=12, fontweight='bold')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n Figure saved to 'training_curves.png'\")\n",
    "plt.show()\n",
    "\n",
    "# ===== Additional: Summary Table Plot =====\n",
    "fig2, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Prepare summary data\n",
    "summary_data = [\n",
    "    ['Metric', 'Best Value', 'Iteration', 'Final Avg', 'Std'],\n",
    "    ['Test Loss', f'{best_test_loss:.4f}', f'{best_test_loss_iter:,}',\n",
    "     f'{final_loss.mean():.4f}', f'±{final_loss.std():.4f}'],\n",
    "    ['Test Accuracy', f'{100*best_test_acc:.2f}%', f'{best_test_acc_iter:,}',\n",
    "     f'{100*final_acc.mean():.2f}%', f'±{100*final_acc.std():.2f}%'],\n",
    "    ['Last Char Acc', f'{100*best_test_acc_last:.2f}%', f'{best_test_acc_last_iter:,}',\n",
    "     f'{100*final_acc_last.mean():.2f}%', f'±{100*final_acc_last.std():.2f}%'],\n",
    "]\n",
    "\n",
    "table = ax.table(cellText=summary_data, cellLoc='center', loc='center',\n",
    "                colWidths=[0.25, 0.2, 0.2, 0.2, 0.15])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style the header row\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(summary_data)):\n",
    "    for j in range(5):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#f0f0f0')\n",
    "\n",
    "plt.title('Performance Summary Table', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1\n",
    "seed = 4212\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "prompt = \"hello my fri\"\n",
    "# prompt_int = encode(prompt.lower())\n",
    "prompt_int = jnp.array([[char_to_int.get(c, len(char_set)) for c in prompt.lower()[:64]]], dtype=jnp.int32)\n",
    "\n",
    "gen_len = 1000\n",
    "out_ids = generation.generate_tokens(model, params, rng, prompt_int, gen_len, block_size=64,\n",
    "                          temperature=0.7, sample=True)\n",
    "print('generated ids shape:', out_ids.shape)\n",
    "print('generated text:')\n",
    "generated_text = ''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0]))\n",
    "# concatenate with prompt\n",
    "print(prompt + generated_text)\n",
    "# print(''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charllm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
