{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2def0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload of local Python modules (e.g., models)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5001b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import time\n",
    "\n",
    "# local imports\n",
    "import models.models as models\n",
    "import util.generation as generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906db4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the jax random key\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a1c8f",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86825275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ./data/text8_train.txt and ./data/text8_test.txt files\n",
    "with open(\"./data/text8_train.txt\", \"r\") as f:\n",
    "    train_text = f.read()\n",
    "with open(\"./data/text8_test.txt\", \"r\") as f:\n",
    "    test_text = f.read()\n",
    "\n",
    "# print the length of the training text and test text\n",
    "print(f\"Length of training text: {len(train_text):_} characters\")\n",
    "print(f\"Length of test text: {len(test_text):_} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdca63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary (lowercase + space + a few punctuations)\n",
    "char_set = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "char_to_int = {ch:i for i,ch in enumerate(char_set)}\n",
    "int_to_char = {i:ch for ch,i in char_to_int.items()}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"Encode string to array of integers\"\"\"\n",
    "    ids = [char_to_int[c] for c in s]\n",
    "    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text\n",
    "train_text_int = encode(train_text)\n",
    "test_text_int = encode(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: display a few random characters from the training text\n",
    "T = 128\n",
    "for _ in range(5):\n",
    "    # choose random position in text\n",
    "    N = np.random.randint(low=0, high=len(train_text)-T)\n",
    "    print(train_text[N:N+T])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724c34b",
   "metadata": {},
   "source": [
    "# Create a basic Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5955c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, vocab_size=27, d_model=64, n_layers=6, n_heads=8, max_len=128):\n",
    "    # create a basic Transformer model\n",
    "    model = models.DecoderOnlyTransformer(vocab_size, d_model, n_layers, n_heads, max_len)\n",
    "    # create a dummy input for initialization\n",
    "    dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)\n",
    "    # pass the dummy input to the model to initialize the parameters\n",
    "    params = model.init({\"params\": rng}, dummy)[\"params\"]\n",
    "    return model, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size\n",
    "vocab_size = len(char_set)\n",
    "\n",
    "# internal model dimensions\n",
    "d_model = 256\n",
    "\n",
    "# number of attention heads\n",
    "n_heads = 8\n",
    "\n",
    "# number of Transformer layers\n",
    "n_layers = 2\n",
    "\n",
    "# maximum sequence length\n",
    "max_len = 128\n",
    "\n",
    "model, params = create_train_state(key, vocab_size, d_model, n_layers, n_heads, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of parameters\n",
    "def count_params(params):\n",
    "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "\n",
    "print(f\"Number of parameters: {count_params(params):_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52231d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: create a batch of data & run a forward pass\n",
    "B, T = 4, 32\n",
    "batch = jax.random.randint(\n",
    "    key=key,\n",
    "    shape=(B, T), minval=0, maxval=len(char_set))\n",
    "logits = model.apply({\"params\": params}, batch)\n",
    "\n",
    "print(\"batch shape:\", batch.shape)  # (B, T)\n",
    "print(\"logits shape:\", logits.shape)  # (B, T, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0213425",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_and_metrics(logits, targets):\n",
    "    \"\"\"Compute cross-entropy loss and accuracy.\n",
    "\n",
    "    Assumes `targets` contains only valid integer class ids in [0, V-1] (no -1 ignore tokens).\n",
    "\n",
    "    Args:\n",
    "      logits: (B, T, V) float array of unnormalized scores.\n",
    "      targets: (B, T) integer array with ground-truth class ids.\n",
    "\n",
    "    Returns:\n",
    "      loss: scalar average cross-entropy over all positions.\n",
    "      metrics: dict with keys \"loss\" and \"acc\" (both scalars).\n",
    "    \"\"\"\n",
    "    # Flatten batch/time dims so optax works on shape (N, V) and (N,)\n",
    "    vocab = logits.shape[-1]\n",
    "    flat_logits = logits.reshape(-1, vocab)\n",
    "    flat_targets = targets.reshape(-1)\n",
    "\n",
    "    # Per-position cross-entropy, then mean over all positions\n",
    "    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)\n",
    "    loss = per_pos.mean()\n",
    "\n",
    "    # prediction over all positions\n",
    "    preds = jnp.argmax(logits, axis=-1)  # (B, T)\n",
    "\n",
    "    # compute accuracy over only the last position\n",
    "    is_match = preds == targets\n",
    "\n",
    "    # Accuracy over all positions\n",
    "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
    "\n",
    "    # Accuracy over only last position\n",
    "    acc_last = jnp.mean(is_match.astype(jnp.float32)[:,-1])\n",
    "\n",
    "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05221147",
   "metadata": {},
   "source": [
    "# Optimization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an update function\n",
    "def train_step(params, opt_state, x, y, tx):\n",
    "    \"\"\"Single optimization step using optax optimizer.\n",
    "\n",
    "    Args:\n",
    "      params: pytree of model parameters.\n",
    "      opt_state: optax optimizer state corresponding to `params`.\n",
    "      x: (B, T) int array input tokens.\n",
    "      y: (B, T) int array target tokens.\n",
    "      tx: optax.GradientTransformation (already initialized).\n",
    "\n",
    "    Returns:\n",
    "      new_params: updated parameters after one gradient step.\n",
    "      new_opt_state: updated optimizer state.\n",
    "      metrics: dict of scalar metrics (loss, acc).\n",
    "    \"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply({\"params\": params}, x)\n",
    "        loss, metrics = loss_and_metrics(logits, y)\n",
    "        return loss, metrics\n",
    "\n",
    "    # compute gradients (loss is scalar, metrics is auxiliary)\n",
    "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "\n",
    "    # optax update: compute parameter updates and new optimizer state\n",
    "    updates, new_opt_state = tx.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, metrics\n",
    "\n",
    "\n",
    "# jit: last argument should be static because it is an object\n",
    "train_step = jax.jit(train_step, static_argnames=(\"tx\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d8e59",
   "metadata": {},
   "source": [
    "# Batch creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb30f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch from the training data\n",
    "def get_batch(text_int, B, T):\n",
    "    \"\"\"Create a random batch of data from text_int.\n",
    "\n",
    "    Args:\n",
    "      text_int: 1D array of token ids.\n",
    "      B: batch size (number of sequences).\n",
    "      T: sequence length (number of tokens per sequence).\n",
    "\n",
    "    Returns:\n",
    "      x: (B, T) int array input tokens.\n",
    "      y: (B, T) int array target tokens.\n",
    "    \"\"\"\n",
    "    # choose random starting indices for each sequence in the batch\n",
    "    ix = np.random.randint(0, len(text_int) - T, size=B)\n",
    "    # inputs are text from i to i+T\n",
    "    x = np.stack([text_int[i:i+T] for i in ix])\n",
    "    # targets are text from i+1 to i+T+1\n",
    "    y = np.stack([text_int[i+1:i+T+1] for i in ix])\n",
    "    return jnp.array(x, dtype=jnp.int32), jnp.array(y, dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da80db",
   "metadata": {},
   "source": [
    "# Optimizer creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 100_000\n",
    "B, T = 128, 32\n",
    "warmup_steps = 2_000\n",
    "decay_steps = max(1, niter - warmup_steps)\n",
    "\n",
    "# define optax optimizer\n",
    "learning_rate = 0.001\n",
    "\n",
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_steps=decay_steps,\n",
    "    end_value=0.0,\n",
    ")\n",
    "\n",
    "# Create Adam optimizer (Optax)\n",
    "# tx = optax.adam(learning_rate=learning_rate)\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),    # Optional gradient clipping\n",
    "    optax.adamw(\n",
    "        learning_rate=lr_schedule,     # ‚Üê Use schedule here\n",
    "        b1=0.9,\n",
    "        b2=0.95,\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.1,\n",
    "    ),\n",
    ")\n",
    "# Initialize optimizer state for current params\n",
    "opt_state = tx.init(params)\n",
    "print(f\"Initialized optimizer: Adam lr={learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "time_history = []\n",
    "time_test_history = []\n",
    "loss_test_history = []\n",
    "time_start = time.time()\n",
    "for it in range(niter):\n",
    "    batch = get_batch(train_text_int, B, T)\n",
    "    input, target = batch[0], batch[1]\n",
    "    params_new, opt_state_new, metrics = train_step(params, opt_state, input, target, tx)\n",
    "\n",
    "    # update params and opt_state\n",
    "    params = params_new\n",
    "    opt_state = opt_state_new\n",
    "    acc = metrics['acc']\n",
    "    acc_last = metrics['acc_last']\n",
    "    loss = metrics['loss']\n",
    "\n",
    "    loss_history.append(loss)\n",
    "    time_history.append(time.time() - time_start)\n",
    "\n",
    "    if it % (niter // 50) == 0 or it == niter - 1:\n",
    "        time_since_start = time.time() - time_start\n",
    "        # compute loss on test set\n",
    "        B_test, T_test = 1024, 32\n",
    "        test_batch = get_batch(test_text_int, B_test, T_test)\n",
    "        test_input, test_target = test_batch[0], test_batch[1]\n",
    "        test_logits = model.apply({\"params\": params}, test_input)\n",
    "        test_loss, test_metrics = loss_and_metrics(test_logits, test_target)\n",
    "        test_acc = test_metrics['acc']\n",
    "        test_acc_last = test_metrics['acc_last']\n",
    "        loss_test_history.append(test_loss)\n",
    "        time_test_history.append(time_since_start)\n",
    "        print(f\"iteration {it:_}  time: {time_since_start:.1f} seconds\")\n",
    "        print(f\"\\t \\t loss(train :: test): {loss:.4f} :: {test_loss:.4f}\")\n",
    "        print(f\"\\t \\t accuracy (train :: test): {100*acc:.1f}% :: {100*test_acc:.1f}%\")\n",
    "        print(f\"\\t \\t accuracy (last character) (train :: test): {100*acc_last:.1f}% :: {100*test_acc_last:.1f}%\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss history\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(time_history, loss_history, '-', label='train', color=\"blue\")\n",
    "plt.plot(time_test_history, loss_test_history, '-', label='test', lw=2, color=\"red\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1\n",
    "seed = 42\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "prompt = \"hello my fri\"\n",
    "# prompt_int = encode(prompt.lower())\n",
    "prompt_int = jnp.array([[char_to_int.get(c, len(char_set)) for c in prompt.lower()[:64]]], dtype=jnp.int32)\n",
    "\n",
    "gen_len = 1000\n",
    "out_ids = generation.generate_tokens(model, params, rng, prompt_int, gen_len, block_size=64,\n",
    "                          temperature=0.7, sample=True)\n",
    "print('generated ids shape:', out_ids.shape)\n",
    "print('generated text:')\n",
    "generated_text = ''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0]))\n",
    "# concatenate with prompt\n",
    "print(prompt + generated_text)\n",
    "# print(''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charllm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
